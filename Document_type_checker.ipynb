{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫"
      ],
      "metadata": {
        "id": "xEQT_vpDRrvu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VPoZaO5iV08v"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import json\n",
        "from pathlib import Path\n",
        "import re\n",
        "import os\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas.io.json import json_normalize\n",
        "\n",
        "import requests\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from striprtf.striprtf import rtf_to_text\n",
        "\n",
        "from pymystem3 import Mystem\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –§—É–Ω–∫—Ü–∏–∏ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–æ–≤"
      ],
      "metadata": {
        "id": "_VDt8kqNR09v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_full_RTFtext(filename):      # –§—É–Ω–∫—Ü–∏—è –¥–ª—è —á—Ç–µ–Ω–∏—è .rtf —Ñ–∞–π–ª–æ–≤ \n",
        "    with open(filename) as infile:\n",
        "        content = infile.read()\n",
        "        text = rtf_to_text(content)\n",
        "\n",
        "    return text\n",
        "\n",
        "def get_text(filename):               # –§—É–Ω–∫—Ü–∏—è –¥–ª—è —á—Ç–µ–Ω–∏—è –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ \n",
        "    with open(filename, 'rb') as f:\n",
        "        encoded = base64.b64encode(f.read())\n",
        "\n",
        "    ext = Path(filename).suffix[1:]\n",
        "    if ext == 'rtf':\n",
        "        return get_full_RTFtext(filename)\n",
        "\n",
        "    url = 'http://localhost:8080/textract'\n",
        "    myobj = {\n",
        "        'data': encoded.decode('utf-8'),\n",
        "        'file_type': ext\n",
        "    }\n",
        "\n",
        "    x = requests.post(url, json = myobj)\n",
        "\n",
        "    if x.status_code != 200:\n",
        "        return None\n",
        "        \n",
        "    return json.loads(x.text)['text']"
      ],
      "metadata": {
        "id": "MamY0Nk0WAoG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥"
      ],
      "metadata": {
        "id": "gwJIe5v_SGP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(text):\n",
        "    mystem = Mystem()\n",
        "\n",
        "    tokens = mystem.lemmatize(text)\n",
        "    tokens = [token for token in tokens if\n",
        "              token not in [\" \", '\\n'] \\\n",
        "              ]\n",
        "    text = ' '.join(tokens)\n",
        "    return text\n",
        "\n",
        "\n",
        "def preprocess(my_string, verbose=True):\n",
        "    if verbose:\n",
        "        print('-', end='')\n",
        "    \n",
        "    my_string = re.sub(r\"http\\S+\", \"\", my_string)\n",
        "    my_string = re.sub(r'(\\!)(.*?)\\.((png)|(jpg)|(jpeg))', \"\", my_string)\n",
        "    my_string = re.sub(r'(\\{color)(.*?)\\}', \"\", my_string)\n",
        "    my_string = re.sub(r'.*?\\!(.*)!.*', \"\", my_string)\n",
        "    \n",
        "    my_string = my_string.lower().strip()\n",
        "\n",
        "    my_string = re.sub(r'[^\\w\\s]', ' ', my_string)\n",
        "    my_string = re.sub(r'\\n', ' ', my_string)\n",
        "    my_string = re.sub(r'[0-9]+', '', my_string)\n",
        "    my_string = re.sub(r'  *', ' ', my_string)\n",
        "    my_string = my_string.strip()\n",
        "    my_string = normalize(my_string)\n",
        "    my_string = re.sub( r'  *', ' ', my_string)\n",
        "    my_string = ' '.join([el for el in my_string.strip().split() if len(el) > 1])\n",
        "    my_string = my_string.replace('_', '')\n",
        "    \n",
        "    return my_string"
      ],
      "metadata": {
        "id": "kIpoodIMWH10"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read(filepath):\n",
        "    text = get_text(filepath).replace('\\n', ' ')\n",
        "    return text"
      ],
      "metadata": {
        "id": "LbWWT30QWKEs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DocumentTypeChecker: –±–æ—Ç"
      ],
      "metadata": {
        "id": "mP9B55jySRyQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"
      ],
      "metadata": {
        "id": "UkmRmatXSgXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = pickle.load(open('D:/Downloads/hacka/tfidf.pkl', 'rb')) # –∑–∞–≥—Ä—É–∑–∫–∞ tfidf\n",
        "model = pickle.load(open('D:/Downloads/hacka/logistic_model.pkl', 'rb'))# –∑–∞–≥—Ä—É–∑–∫–∞ LogisticRegression\n",
        "le = pickle.load(open('D:/Downloads/hacka/LabelEncoder.pkl', 'rb'))# –∑–∞–≥—Ä—É–∑–∫–∞ LabelEncoder"
      ],
      "metadata": {
        "id": "bc1yd5IltlNT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "with open('C:/Users/–ü–∞–≤–µ–ª/Downloads/Telegram Desktop/keyphrases2.txt', encoding='utf-8') as f:\n",
        "    keyphrases = f.readlines()\n",
        "\n",
        "processed_keyphrases = preprocess(' –¥–¥–¥ '.join(keyphrases)).split(' –¥–¥–¥ ')\n",
        "\n",
        "proc2orig = defaultdict(list)\n",
        "for idx, keyph in enumerate(processed_keyphrases):\n",
        "    proc2orig[keyph].append(keyphrases[idx].strip())\n",
        "\n",
        "def get_orig_from_processed(words, text):\n",
        "    orig_words = []\n",
        "    for word in words:\n",
        "        for orig_word in proc2orig[word]:\n",
        "            if orig_word in text:\n",
        "                orig_words.append(orig_word)\n",
        "                break\n",
        "    return orig_words\n",
        "\n",
        "def find_ngrams(input_list, n):\n",
        "    return zip(*[input_list[i:] for i in range(n)])\n",
        "\n",
        "def get_context_from_words(phrases, text, context_window=2):\n",
        "    result = []\n",
        "    tokens = text.split()\n",
        "    for phrase in phrases:\n",
        "        phrase_words = phrase.split()\n",
        "        for idx, ngram in enumerate(find_ngrams(tokens, len(phrase_words))):\n",
        "            if list(ngram) == phrase_words:\n",
        "                span_begin = max(0, idx - context_window)\n",
        "                span_end = min(len(tokens), idx + len(phrase_words) + context_window)\n",
        "                result.append(' '.join(tokens[span_begin:span_end]))\n",
        "    result = random.sample(result, k=min(10, len(result)))\n",
        "    return result\n",
        "\n",
        "def end2end(filepath):                                # —Ñ—É–Ω–∫—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
        "    orig_text = read(filepath)\n",
        "    text = preprocess(orig_text, verbose=False)\n",
        "    vec = tfidf.transform([text])\n",
        "    \n",
        "    result = {}\n",
        "    result['doc_id'] = Path(filepath).stem\n",
        "    result['num_class'] = model.predict(vec)[0]\n",
        "    result['text_class'] = le.classes_[result['num_class']]\n",
        "    result['confidence'] = model.predict_proba(vec)[0, result['num_class']]\n",
        "    \n",
        "\n",
        "    nums = vec[0].toarray() * model.coef_[result['num_class']]\n",
        "    top = np.argpartition(nums, -10)[:, -10:]\n",
        "\n",
        "    proc_words = tfidf.get_feature_names_out()[top].reshape(-1).tolist()\n",
        "    orig_words = get_orig_from_processed(proc_words, orig_text)\n",
        "    result['top_words'] = ', '.join(orig_words)\n",
        "    result['context_phrases'] = get_context_from_words(orig_words, orig_text)\n",
        "    return result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO8pC4RkWaGO",
        "outputId": "c1449983-2fd8-4330-ff8a-5a24ce34cd91"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞ "
      ],
      "metadata": {
        "id": "-6F8EXJfSwtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import telebot\n",
        "\n",
        "TOKEN = '5776246544:AAH0RhhoPz91lIsZeRH5xpR-5kCmC4I-OAs'\n",
        "URL = 'https://api.telegram.org/bot'\n",
        "bot = telebot.TeleBot(TOKEN)\n",
        "\n",
        "from telebot import types\n",
        "from telebot.types import InputFile\n",
        "\n",
        "file_cache = None\n",
        "result_cache = None\n",
        "\n",
        "\n",
        "@bot.message_handler(commands=['start'])\n",
        "def start(message):\n",
        "  bot.send_message(message.from_user.id, \"üëã –ü—Ä–∏–≤–µ—Ç! –Ø —Ç–≤–æ–π –±–æ—Ç-–ø–æ–º–æ—à–Ω–∏–∫! –û—Ç–ø—Ä–∞–≤—å –º–Ω–µ –¥–æ–≥–æ–≤–æ—Ä –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è\")\n",
        "\n",
        "@bot.message_handler(content_types=['document'])\n",
        "def handle_docs_doc(message):\n",
        "  try:\n",
        "      chat_id = message.chat.id\n",
        "      file_info = bot.get_file(message.document.file_id)\n",
        "      downloaded_file = bot.download_file(file_info.file_path)\n",
        "\n",
        "      src = os.path.join('D:/Downloads/', message.document.file_name) \n",
        "\n",
        "      if src.split('.')[1] in ['docx','doc','rtf','pdf']: \n",
        "        with open(src, 'wb') as new_file:\n",
        "            new_file.write(downloaded_file)\n",
        "            global file_cache\n",
        "            file_cache = src\n",
        "\n",
        "\n",
        "        result = end2end(src)\n",
        "        global result_cache\n",
        "        result_cache = result\n",
        "\n",
        "        bot.reply_to(message, f\"*–î–æ–∫—É–º–µ–Ω—Ç:* {result['doc_id']}\\n\\n\"\n",
        "                              f\"*–ö–ª–∞—Å—Å:* {result['text_class']}\\n\\n\"\n",
        "                              f\"*–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:* {np.round(result['confidence'] * 100, 2)}%\\n\\n\"\n",
        "                              f\"*–ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞:* {result['top_words']}\", parse_mode=\"Markdown\"\n",
        "                              )\n",
        "        backslash_char = \"\\n\"\n",
        "        bot.reply_to(message, f\"*–ö–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã:*{f'{backslash_char*2}'} {f'{backslash_char*2}'.join(result['context_phrases'])}\", parse_mode=\"Markdown\")\n",
        "        \n",
        "        markup = types.ReplyKeyboardMarkup(resize_keyboard=True)\n",
        "        btn1 = types.KeyboardButton(\"üëã –û—Ç–ø—Ä–∞–≤–∏—Ç—å –≤ –æ–±—Ä–∞–±–æ—Ç–∫—É\")\n",
        "        btn2 = types.KeyboardButton(\"‚ùì –°–æ–æ–±—â–∏—Ç—å –≤ –ø–æ–¥–¥–µ—Ä–∂–∫—É\")\n",
        "        markup.add(btn1, btn2) \n",
        "        bot.send_message(message.from_user.id, \"–í—ã–±–µ—Ä–∏—Ç–µ –æ–ø—Ü–∏—é\", reply_markup = markup)\n",
        "      \n",
        "      else:\n",
        "        bot.send_message(message.from_user.id, \"–£–ø—Å... –ù–µ–≤–µ—Ä–Ω—ã–π —Ç–∏–ø —Ñ–∞–π–ª–∞. –í—ã–±–µ—Ä–∏—Ç–µ —Ñ–∞–π–ª –∏–∑ —Å–ø–∏—Å–∫–∞ –¥–æ–ø—É—Å—Ç–∏–º—ã—Ö —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–π: DOCX, DOC, PDF, RTF\")\n",
        "          \n",
        "  except Exception as e:\n",
        "      bot.reply_to(message, e)\n",
        "\n",
        "\n",
        "@bot.message_handler(content_types=['text'])\n",
        "def func(message):\n",
        "    global result_cache\n",
        "    global file_cache\n",
        "    if(message.text == \"üëã –û—Ç–ø—Ä–∞–≤–∏—Ç—å –≤ –æ–±—Ä–∞–±–æ—Ç–∫—É\"):\n",
        "\n",
        "      if result_cache['text_class'] == '–î–æ–≥–æ–≤–æ—Ä—ã –¥–ª—è –∞–∫—Å–µ–ª–µ—Ä–∞—Ç–æ—Ä–∞/–î–æ–≥–æ–≤–æ—Ä—ã –æ–∫–∞–∑–∞–Ω–∏—è —É—Å–ª—É–≥':\n",
        "        bot.send_document('@EmbedikaServiceContracts', InputFile(file_cache))\n",
        "        bot.send_message(message.chat.id, text=\"–î–æ–∫—É–º–µ–Ω—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –≤ –æ–±—Ä–∞–±–æ—Ç–∫—É\")\n",
        "\n",
        "      elif result_cache['text_class'] == '–î–æ–≥–æ–≤–æ—Ä—ã –¥–ª—è –∞–∫—Å–µ–ª–µ—Ä–∞—Ç–æ—Ä–∞/–î–æ–≥–æ–≤–æ—Ä—ã –ø–æ–¥—Ä—è–¥–∞':\n",
        "        bot.send_document('@EmbedikaContractAgreements', InputFile(file_cache))\n",
        "        bot.send_message(message.chat.id, text=\"–î–æ–∫—É–º–µ–Ω—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –≤ –æ–±—Ä–∞–±–æ—Ç–∫—É\")\n",
        "\n",
        "      elif result_cache['text_class'] == '–î–æ–≥–æ–≤–æ—Ä—ã –¥–ª—è –∞–∫—Å–µ–ª–µ—Ä–∞—Ç–æ—Ä–∞/–î–æ–≥–æ–≤–æ—Ä—ã –∫—É–ø–ª–∏-–ø—Ä–æ–¥–∞–∂–∏':\n",
        "        bot.send_document('@EmbedikaPurchaseAndSaleAgreement', InputFile(file_cache))\n",
        "        bot.send_message(message.chat.id, text=\"–î–æ–∫—É–º–µ–Ω—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –≤ –æ–±—Ä–∞–±–æ—Ç–∫—É\")\n",
        "\n",
        "      elif result_cache['text_class'] == '–î–æ–≥–æ–≤–æ—Ä—ã –¥–ª—è –∞–∫—Å–µ–ª–µ—Ä–∞—Ç–æ—Ä–∞/–î–æ–≥–æ–≤–æ—Ä—ã –∞—Ä–µ–Ω–¥—ã':\n",
        "        bot.send_document('@EmbedikaLeaseAgreements', InputFile(file_cache))\n",
        "        bot.send_message(message.chat.id, text=\"–î–æ–∫—É–º–µ–Ω—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –≤ –æ–±—Ä–∞–±–æ—Ç–∫—É\")\n",
        "\n",
        "      elif result_cache['text_class'] == '–î–æ–≥–æ–≤–æ—Ä—ã –¥–ª—è –∞–∫—Å–µ–ª–µ—Ä–∞—Ç–æ—Ä–∞/–î–æ–≥–æ–≤–æ—Ä—ã –ø–æ—Å—Ç–∞–≤–∫–∏':\n",
        "        bot.send_document('@EmbedikaSupplyContracts', InputFile(file_cache))\n",
        "        bot.send_message(message.chat.id, text=\"–î–æ–∫—É–º–µ–Ω—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –≤ –æ–±—Ä–∞–±–æ—Ç–∫—É\")\n",
        "\n",
        "      else:\n",
        "        bot.send_message(message.from_user.id, \"–£–ø—Å... –î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –≤ –æ–±—Ä–∞–±–æ—Ç–∫—É\")\n",
        "\n",
        "    elif(message.text == \"‚ùì –°–æ–æ–±—â–∏—Ç—å –≤ –ø–æ–¥–¥–µ—Ä–∂–∫—É\"):\n",
        "      bot.send_message('@EmbedikaTechnicalSupport', f\"–ù–ï–í–ï–†–ù–ê–Ø –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–Ø!!!\\n\"\n",
        "                                                    f\"–î–æ–∫—É–º–µ–Ω—Ç: {result_cache['doc_id']}\\n\"\n",
        "                                                    f\"–ö–ª–∞—Å—Å: {result_cache['text_class']}\\n\"\n",
        "                                                    f\"–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {np.round(result_cache['confidence'] * 100, 2)}%\\n\"\n",
        "                                                    f\"–ö–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã: {result_cache['top_words']}\"\n",
        "                                                    )\n",
        "      bot.send_document('@EmbedikaTechnicalSupport', InputFile(file_cache))\n",
        "      bot.send_message(message.chat.id, text=\"–î–æ–∫—É–º–µ–Ω—Ç –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –≤ —Å–ª—É–∂–±—É –ø–æ–¥–¥–µ—Ä–∂–∫–∏\")\n",
        "\n",
        "\n",
        "\n",
        "bot.polling(none_stop=True, interval=0)"
      ],
      "metadata": {
        "id": "bq2Bo8ZFWoib"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}