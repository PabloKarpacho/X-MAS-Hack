{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Импорт библиотек"
      ],
      "metadata": {
        "id": "xEQT_vpDRrvu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VPoZaO5iV08v"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import json\n",
        "from pathlib import Path\n",
        "import re\n",
        "import os\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas.io.json import json_normalize\n",
        "\n",
        "import requests\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from striprtf.striprtf import rtf_to_text\n",
        "\n",
        "from pymystem3 import Mystem\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import f1_score, recall_score, precision_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Функции чтения файлов"
      ],
      "metadata": {
        "id": "_VDt8kqNR09v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_full_RTFtext(filename):      # Функция для чтения .rtf файлов \n",
        "    with open(filename) as infile:\n",
        "        content = infile.read()\n",
        "        text = rtf_to_text(content)\n",
        "\n",
        "    return text\n",
        "\n",
        "def get_text(filename):               # Функция для чтения всех остальных файлов \n",
        "    with open(filename, 'rb') as f:\n",
        "        encoded = base64.b64encode(f.read())\n",
        "\n",
        "    ext = Path(filename).suffix[1:]\n",
        "    if ext == 'rtf':\n",
        "        return get_full_RTFtext(filename)\n",
        "\n",
        "    url = 'http://localhost:8080/textract'\n",
        "    myobj = {\n",
        "        'data': encoded.decode('utf-8'),\n",
        "        'file_type': ext\n",
        "    }\n",
        "\n",
        "    x = requests.post(url, json = myobj)\n",
        "\n",
        "    if x.status_code != 200:\n",
        "        return None\n",
        "        \n",
        "    return json.loads(x.text)['text']"
      ],
      "metadata": {
        "id": "MamY0Nk0WAoG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Препроцессинг"
      ],
      "metadata": {
        "id": "gwJIe5v_SGP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(text):\n",
        "    mystem = Mystem()\n",
        "\n",
        "    tokens = mystem.lemmatize(text)\n",
        "    tokens = [token for token in tokens if\n",
        "              token not in [\" \", '\\n'] \\\n",
        "              ]\n",
        "    text = ' '.join(tokens)\n",
        "    return text\n",
        "\n",
        "\n",
        "def preprocess(my_string, verbose=True):\n",
        "    if verbose:\n",
        "        print('-', end='')\n",
        "    \n",
        "    my_string = re.sub(r\"http\\S+\", \"\", my_string)\n",
        "    my_string = re.sub(r'(\\!)(.*?)\\.((png)|(jpg)|(jpeg))', \"\", my_string)\n",
        "    my_string = re.sub(r'(\\{color)(.*?)\\}', \"\", my_string)\n",
        "    my_string = re.sub(r'.*?\\!(.*)!.*', \"\", my_string)\n",
        "    \n",
        "    my_string = my_string.lower().strip()\n",
        "\n",
        "    my_string = re.sub(r'[^\\w\\s]', ' ', my_string)\n",
        "    my_string = re.sub(r'\\n', ' ', my_string)\n",
        "    my_string = re.sub(r'[0-9]+', '', my_string)\n",
        "    my_string = re.sub(r'  *', ' ', my_string)\n",
        "    my_string = my_string.strip()\n",
        "    my_string = normalize(my_string)\n",
        "    my_string = re.sub( r'  *', ' ', my_string)\n",
        "    my_string = ' '.join([el for el in my_string.strip().split() if len(el) > 1])\n",
        "    my_string = my_string.replace('_', '')\n",
        "    \n",
        "    return my_string"
      ],
      "metadata": {
        "id": "kIpoodIMWH10"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read(filepath):\n",
        "    text = get_text(filepath).replace('\\n', ' ')\n",
        "    return text"
      ],
      "metadata": {
        "id": "LbWWT30QWKEs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DocumentTypeChecker: бот"
      ],
      "metadata": {
        "id": "mP9B55jySRyQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузка обученных моделей"
      ],
      "metadata": {
        "id": "UkmRmatXSgXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = pickle.load(open('D:/Downloads/hacka/tfidf.pkl', 'rb')) # загрузка tfidf\n",
        "model = pickle.load(open('D:/Downloads/hacka/logistic_model.pkl', 'rb'))# загрузка LogisticRegression\n",
        "le = pickle.load(open('D:/Downloads/hacka/LabelEncoder.pkl', 'rb'))# загрузка LabelEncoder"
      ],
      "metadata": {
        "id": "bc1yd5IltlNT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "with open('C:/Users/Павел/Downloads/Telegram Desktop/keyphrases2.txt', encoding='utf-8') as f:\n",
        "    keyphrases = f.readlines()\n",
        "\n",
        "processed_keyphrases = preprocess(' ддд '.join(keyphrases)).split(' ддд ')\n",
        "\n",
        "proc2orig = defaultdict(list)\n",
        "for idx, keyph in enumerate(processed_keyphrases):\n",
        "    proc2orig[keyph].append(keyphrases[idx].strip())\n",
        "\n",
        "def get_orig_from_processed(words, text):\n",
        "    orig_words = []\n",
        "    for word in words:\n",
        "        for orig_word in proc2orig[word]:\n",
        "            if orig_word in text:\n",
        "                orig_words.append(orig_word)\n",
        "                break\n",
        "    return orig_words\n",
        "\n",
        "def find_ngrams(input_list, n):\n",
        "    return zip(*[input_list[i:] for i in range(n)])\n",
        "\n",
        "def get_context_from_words(phrases, text, context_window=2):\n",
        "    result = []\n",
        "    tokens = text.split()\n",
        "    for phrase in phrases:\n",
        "        phrase_words = phrase.split()\n",
        "        for idx, ngram in enumerate(find_ngrams(tokens, len(phrase_words))):\n",
        "            if list(ngram) == phrase_words:\n",
        "                span_begin = max(0, idx - context_window)\n",
        "                span_end = min(len(tokens), idx + len(phrase_words) + context_window)\n",
        "                result.append(' '.join(tokens[span_begin:span_end]))\n",
        "    result = random.sample(result, k=min(10, len(result)))\n",
        "    return result\n",
        "\n",
        "def end2end(filepath):                                # функция результата визуализации\n",
        "    orig_text = read(filepath)\n",
        "    text = preprocess(orig_text, verbose=False)\n",
        "    vec = tfidf.transform([text])\n",
        "    \n",
        "    result = {}\n",
        "    result['doc_id'] = Path(filepath).stem\n",
        "    result['num_class'] = model.predict(vec)[0]\n",
        "    result['text_class'] = le.classes_[result['num_class']]\n",
        "    result['confidence'] = model.predict_proba(vec)[0, result['num_class']]\n",
        "    \n",
        "\n",
        "    nums = vec[0].toarray() * model.coef_[result['num_class']]\n",
        "    top = np.argpartition(nums, -10)[:, -10:]\n",
        "\n",
        "    proc_words = tfidf.get_feature_names_out()[top].reshape(-1).tolist()\n",
        "    orig_words = get_orig_from_processed(proc_words, orig_text)\n",
        "    result['top_words'] = ', '.join(orig_words)\n",
        "    result['context_phrases'] = get_context_from_words(orig_words, orig_text)\n",
        "    return result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO8pC4RkWaGO",
        "outputId": "c1449983-2fd8-4330-ff8a-5a24ce34cd91"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Реализация бота "
      ],
      "metadata": {
        "id": "-6F8EXJfSwtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import telebot\n",
        "\n",
        "TOKEN = '5776246544:AAH0RhhoPz91lIsZeRH5xpR-5kCmC4I-OAs'\n",
        "URL = 'https://api.telegram.org/bot'\n",
        "bot = telebot.TeleBot(TOKEN)\n",
        "\n",
        "from telebot import types\n",
        "from telebot.types import InputFile\n",
        "\n",
        "file_cache = None\n",
        "result_cache = None\n",
        "\n",
        "\n",
        "@bot.message_handler(commands=['start'])\n",
        "def start(message):\n",
        "  bot.send_message(message.from_user.id, \"👋 Привет! Я твой бот-помошник! Отправь мне договор для распознавания\")\n",
        "\n",
        "@bot.message_handler(content_types=['document'])\n",
        "def handle_docs_doc(message):\n",
        "  try:\n",
        "      chat_id = message.chat.id\n",
        "      file_info = bot.get_file(message.document.file_id)\n",
        "      downloaded_file = bot.download_file(file_info.file_path)\n",
        "\n",
        "      src = os.path.join('D:/Downloads/', message.document.file_name) \n",
        "\n",
        "      if src.split('.')[1] in ['docx','doc','rtf','pdf']: \n",
        "        with open(src, 'wb') as new_file:\n",
        "            new_file.write(downloaded_file)\n",
        "            global file_cache\n",
        "            file_cache = src\n",
        "\n",
        "\n",
        "        result = end2end(src)\n",
        "        global result_cache\n",
        "        result_cache = result\n",
        "\n",
        "        bot.reply_to(message, f\"*Документ:* {result['doc_id']}\\n\\n\"\n",
        "                              f\"*Класс:* {result['text_class']}\\n\\n\"\n",
        "                              f\"*Уверенность:* {np.round(result['confidence'] * 100, 2)}%\\n\\n\"\n",
        "                              f\"*Ключевые слова:* {result['top_words']}\", parse_mode=\"Markdown\"\n",
        "                              )\n",
        "        backslash_char = \"\\n\"\n",
        "        bot.reply_to(message, f\"*Ключевые фразы:*{f'{backslash_char*2}'} {f'{backslash_char*2}'.join(result['context_phrases'])}\", parse_mode=\"Markdown\")\n",
        "        \n",
        "        markup = types.ReplyKeyboardMarkup(resize_keyboard=True)\n",
        "        btn1 = types.KeyboardButton(\"👋 Отправить в обработку\")\n",
        "        btn2 = types.KeyboardButton(\"❓ Сообщить в поддержку\")\n",
        "        markup.add(btn1, btn2) \n",
        "        bot.send_message(message.from_user.id, \"Выберите опцию\", reply_markup = markup)\n",
        "      \n",
        "      else:\n",
        "        bot.send_message(message.from_user.id, \"Упс... Неверный тип файла. Выберите файл из списка допустимых расширений: DOCX, DOC, PDF, RTF\")\n",
        "          \n",
        "  except Exception as e:\n",
        "      bot.reply_to(message, e)\n",
        "\n",
        "\n",
        "@bot.message_handler(content_types=['text'])\n",
        "def func(message):\n",
        "    global result_cache\n",
        "    global file_cache\n",
        "    if(message.text == \"👋 Отправить в обработку\"):\n",
        "\n",
        "      if result_cache['text_class'] == 'Договоры для акселератора/Договоры оказания услуг':\n",
        "        bot.send_document('@EmbedikaServiceContracts', InputFile(file_cache))\n",
        "        bot.send_message(message.chat.id, text=\"Документ отправлен в обработку\")\n",
        "\n",
        "      elif result_cache['text_class'] == 'Договоры для акселератора/Договоры подряда':\n",
        "        bot.send_document('@EmbedikaContractAgreements', InputFile(file_cache))\n",
        "        bot.send_message(message.chat.id, text=\"Документ отправлен в обработку\")\n",
        "\n",
        "      elif result_cache['text_class'] == 'Договоры для акселератора/Договоры купли-продажи':\n",
        "        bot.send_document('@EmbedikaPurchaseAndSaleAgreement', InputFile(file_cache))\n",
        "        bot.send_message(message.chat.id, text=\"Документ отправлен в обработку\")\n",
        "\n",
        "      elif result_cache['text_class'] == 'Договоры для акселератора/Договоры аренды':\n",
        "        bot.send_document('@EmbedikaLeaseAgreements', InputFile(file_cache))\n",
        "        bot.send_message(message.chat.id, text=\"Документ отправлен в обработку\")\n",
        "\n",
        "      elif result_cache['text_class'] == 'Договоры для акселератора/Договоры поставки':\n",
        "        bot.send_document('@EmbedikaSupplyContracts', InputFile(file_cache))\n",
        "        bot.send_message(message.chat.id, text=\"Документ отправлен в обработку\")\n",
        "\n",
        "      else:\n",
        "        bot.send_message(message.from_user.id, \"Упс... Документ не может быть отправлен в обработку\")\n",
        "\n",
        "    elif(message.text == \"❓ Сообщить в поддержку\"):\n",
        "      bot.send_message('@EmbedikaTechnicalSupport', f\"НЕВЕРНАЯ КЛАССИФИКАЦИЯ!!!\\n\"\n",
        "                                                    f\"Документ: {result_cache['doc_id']}\\n\"\n",
        "                                                    f\"Класс: {result_cache['text_class']}\\n\"\n",
        "                                                    f\"Уверенность: {np.round(result_cache['confidence'] * 100, 2)}%\\n\"\n",
        "                                                    f\"Ключевые фразы: {result_cache['top_words']}\"\n",
        "                                                    )\n",
        "      bot.send_document('@EmbedikaTechnicalSupport', InputFile(file_cache))\n",
        "      bot.send_message(message.chat.id, text=\"Документ отправлен в службу поддержки\")\n",
        "\n",
        "\n",
        "\n",
        "bot.polling(none_stop=True, interval=0)"
      ],
      "metadata": {
        "id": "bq2Bo8ZFWoib"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}